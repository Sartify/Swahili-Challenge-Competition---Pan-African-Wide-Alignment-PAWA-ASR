{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bSkUpL18Hkj-"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## TTS Starter notebook.\n",
        "This is the starter notebook for fine tuning \"microsoft/speecht5_tts\" model with \"facebook/voxpopuli\".\n",
        "\n",
        "The starter includes the following steps.\n",
        "1.   Installation of dependencies\n",
        "2.   Load dataset\n",
        "3.   Pre-processing of data\n",
        "4.   Train model.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Getx5SkZHwhk"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lMCamztuJaG0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Installation"
      ],
      "metadata": {
        "id": "wBa2B5r0JbpH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers datasets soundfile speechbrain accelerate"
      ],
      "metadata": {
        "id": "Jx6kgxHyJhjM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import dependencies"
      ],
      "metadata": {
        "id": "CtL5MJ2fKbSc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "from speechbrain.pretrained import EncoderClassifier\n",
        "from functools import partial\n",
        "from dataclasses import dataclass\n",
        "from typing import Any, Dict, List, Union\n",
        "from transformers import SpeechT5ForTextToSpeech, SpeechT5Processor\n",
        "from transformers import Seq2SeqTrainingArguments\n",
        "from transformers import Seq2SeqTrainer\n",
        "from datasets import load_dataset, Audio\n",
        "from huggingface_hub import notebook_login\n",
        "# from transformers import SpeechT5Processor\n",
        "from collections import defaultdict\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "2iz5x23_JtWX"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RU0bAptPK8ey"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Enable hugging face authentication if you need to push the final model to hugging face."
      ],
      "metadata": {
        "id": "R0NHE_xoJ1pv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "notebook_login()"
      ],
      "metadata": {
        "id": "T9BewN-kJ_sK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4oDy1pLHK9mx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading dataset\n",
        "Since the dataset is huge, let's load only 500 samples."
      ],
      "metadata": {
        "id": "eEit999LJxWK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "dataset = load_dataset(\"facebook/voxpopuli\", \"nl\", split=\"train[:500]\")\n",
        "len(dataset)\n"
      ],
      "metadata": {
        "id": "XEhk3_DTJzxe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "k9ElnG2BLdd7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data processing"
      ],
      "metadata": {
        "id": "T7PVYPGMLwVX"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KVlFWYZNLzDS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sample the audio to 16KHZ\n",
        "Data should be sufficient for fine-tuning. SpeechT5 expects audio data to have a sampling rate of 16 kHz, so let's make sure the dataset meet this requirement:"
      ],
      "metadata": {
        "id": "R4SKfbHKL6tw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        " dataset = dataset.cast_column(\"audio\", Audio(sampling_rate=16000))"
      ],
      "metadata": {
        "id": "Ei3P8BbYMAl7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "abYb_Rs0NK4T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SpeechT5 Data Preprocessing\n",
        "\n",
        "This preprocessing pipeline prepares Dutch audio-text data for SpeechT5 TTS model fine-tuning. The main challenges are handling non-English characters and ensuring proper text normalization.\n",
        "Key Steps\n",
        "1. Load Processor: Initialize SpeechT5 processor containing both tokenizer and feature extractor for data preparation.\n",
        "2. Text Selection: Use normalized_text instead of raw_text because SpeechT5 tokenizer doesn't handle numbers well. The normalized version has numbers written as words.\n",
        "3. Character Compatibility: SpeechT5 was trained on English, so Dutch characters like à, ç, è, ë, í, ï, ö, ü get converted to <unk> tokens, losing meaning.\n",
        "4. Vocabulary Analysis: Extract all unique characters from the dataset and compare with tokenizer vocabulary to identify unsupported characters.\n",
        "5. Character Replacement: Map unsupported Dutch characters to their closest English equivalents (e.g., à → a, ë → e) to preserve meaning while maintaining tokenizer compatibility."
      ],
      "metadata": {
        "id": "VTS5HK87NLUR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Load processor and tokenizer\n",
        "checkpoint = \"microsoft/speecht5_tts\"\n",
        "processor = SpeechT5Processor.from_pretrained(checkpoint)\n",
        "tokenizer = processor.tokenizer\n",
        "\n",
        "# Examine dataset structure\n",
        "dataset[0]\n",
        "# Output shows: raw_text, normalized_text, audio, speaker_id, gender, etc.\n",
        "\n",
        "# Extract all unique characters from dataset\n",
        "def extract_all_chars(batch):\n",
        "    all_text = \" \".join(batch[\"normalized_text\"])\n",
        "    vocab = list(set(all_text))\n",
        "    return {\"vocab\": [vocab], \"all_text\": [all_text]}\n",
        "\n",
        "vocabs = dataset.map(\n",
        "    extract_all_chars,\n",
        "    batched=True,\n",
        "    batch_size=-1,\n",
        "    keep_in_memory=True,\n",
        "    remove_columns=dataset.column_names,\n",
        ")\n",
        "\n",
        "# Compare dataset vocabulary with tokenizer vocabulary\n",
        "dataset_vocab = set(vocabs[\"vocab\"][0])\n",
        "tokenizer_vocab = {k for k, _ in tokenizer.get_vocab().items()}\n",
        "\n",
        "# Find unsupported characters\n",
        "unsupported_chars = dataset_vocab - tokenizer_vocab\n",
        "# Result: {' ', 'à', 'ç', 'è', 'ë', 'í', 'ï', 'ö', 'ü'}\n",
        "\n",
        "# Define character replacements\n",
        "replacements = [\n",
        "    (\"à\", \"a\"),\n",
        "    (\"ç\", \"c\"),\n",
        "    (\"è\", \"e\"),\n",
        "    (\"ë\", \"e\"),\n",
        "    (\"í\", \"i\"),\n",
        "    (\"ï\", \"i\"),\n",
        "    (\"ö\", \"o\"),\n",
        "    (\"ü\", \"u\"),\n",
        "]\n",
        "\n",
        "# Apply text cleanup\n",
        "def cleanup_text(inputs):\n",
        "    for src, dst in replacements:\n",
        "        inputs[\"normalized_text\"] = inputs[\"normalized_text\"].replace(src, dst)\n",
        "    return inputs\n",
        "\n",
        "dataset = dataset.map(cleanup_text)"
      ],
      "metadata": {
        "id": "aecqHhmFNQK4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qJrNibb1N2gu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Speaker Analysis and Embeddings\n",
        "Multi-speaker TTS requires understanding speaker distribution and generating speaker embeddings to differentiate between voices during training.\n",
        "Key Steps\n",
        "1. Speaker Distribution Analysis: Count unique speakers and examples per speaker in the VoxPopuli dataset (500 downloaded sample) to understand data balance.\n",
        "2. Data Filtering: Remove speakers with too few or too many examples to improve training efficiency. Filter to speakers with 1-15 examples for balanced representation.\n",
        "3. Speaker Embeddings: Generate 512-dimensional speaker embeddings using pre-trained SpeechBrain X-vector model to capture voice characteristics.\n",
        "4. Cross-Language Consideration: The X-vector model was trained on English (VoxCeleb), but we're using Dutch data. This may still work reasonably well, though optimal results would require Dutch-trained embeddings."
      ],
      "metadata": {
        "id": "UzGQGLwpN3Fn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Analyze speaker distribution\n",
        "from collections import defaultdict\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "speaker_counts = defaultdict(int)\n",
        "\n",
        "for speaker_id in dataset[\"speaker_id\"]:\n",
        "    speaker_counts[speaker_id] += 1\n",
        "\n",
        "# Visualize speaker distribution\n",
        "plt.figure()\n",
        "plt.hist(speaker_counts.values(), bins=20)\n",
        "plt.ylabel(\"Speakers\")\n",
        "plt.xlabel(\"Examples\")\n",
        "plt.show()\n",
        "\n",
        "# Filter speakers with balanced data (100-400 examples)\n",
        "def select_speaker(speaker_id):\n",
        "    return 1 <= speaker_counts[speaker_id] <= 15\n",
        "\n",
        "dataset = dataset.filter(select_speaker, input_columns=[\"speaker_id\"])\n",
        "\n",
        "# Check filtered results\n",
        "print(f\"Speakers remaining: {len(set(dataset['speaker_id']))}\")\n",
        "print(f\"Examples remaining: {len(dataset)}\")\n",
        "\n",
        "# Create speaker embedding function\n",
        "\n",
        "spk_model_name = \"speechbrain/spkrec-xvect-voxceleb\"\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "speaker_model = EncoderClassifier.from_hparams(\n",
        "    source=spk_model_name,\n",
        "    run_opts={\"device\": device},\n",
        "    savedir=os.path.join(\"/tmp\", spk_model_name),\n",
        ")\n",
        "\n",
        "def create_speaker_embedding(waveform):\n",
        "    with torch.no_grad():\n",
        "        speaker_embeddings = speaker_model.encode_batch(torch.tensor(waveform))\n",
        "        speaker_embeddings = torch.nn.functional.normalize(speaker_embeddings, dim=2)\n",
        "        speaker_embeddings = speaker_embeddings.squeeze().cpu().numpy()\n",
        "    return speaker_embeddings"
      ],
      "metadata": {
        "id": "OrQuf4Y3N8tk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FbuImEtMOC46"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Dataset Processing and Data Collator**\n",
        "Transform the raw dataset into model-ready format with tokenized text, log-mel spectrograms, and speaker embeddings, then create batching functionality for training.\n",
        "Key Steps\n",
        "1. Data Processing: Convert each example using SpeechT5Processor to:\n",
        "\n",
        "Tokenize normalized text into input_ids\n",
        "Convert audio to log-mel spectrogram labels (80 mel bins)\n",
        "Generate 512-dimensional speaker embeddings\n",
        "Create stop_labels for sequence termination\n",
        "\n",
        "2. Length Filtering: Remove examples longer than 200 tokens (originally 600 max) to enable larger batch sizes and prevent memory issues.\n",
        "3. Train/Test Split: Create 90/10 split for training and evaluation.\n",
        "4. Custom Data Collator: Handle variable-length seasquences by:\n",
        "\n",
        "Padding shorter sequences to batch maximum length\n",
        "Masking padded spectrogram areas with -100 (ignored in loss)\n",
        "Adjusting lengths to multiples of reduction factor (2)\n",
        "Batching speaker embeddings as tensors\n"
      ],
      "metadata": {
        "id": "fI0OHfYaODXa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define dataset processing function\n",
        "def prepare_dataset(example):\n",
        "    audio = example[\"audio\"]\n",
        "\n",
        "    example = processor(\n",
        "        text=example[\"normalized_text\"],\n",
        "        audio_target=audio[\"array\"],\n",
        "        sampling_rate=audio[\"sampling_rate\"],\n",
        "        return_attention_mask=False,\n",
        "    )\n",
        "\n",
        "    # Strip off the batch dimension\n",
        "    example[\"labels\"] = example[\"labels\"][0]\n",
        "\n",
        "    # Use SpeechBrain to obtain x-vector\n",
        "    example[\"speaker_embeddings\"] = create_speaker_embedding(audio[\"array\"])\n",
        "\n",
        "    return example\n",
        "\n",
        "# Verify processing on single example\n",
        "processed_example = prepare_dataset(dataset[0])\n",
        "print(list(processed_example.keys()))\n",
        "\n",
        "\n",
        "print(processed_example[\"speaker_embeddings\"].shape)\n",
        "\n",
        "# Visualize log-mel spectrogram\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure()\n",
        "plt.imshow(processed_example[\"labels\"].T)\n",
        "plt.show()\n",
        "# Note: Spectrogram appears upside down due to matplotlib y-axis convention\n",
        "\n",
        "# Apply processing to entire dataset (5-10 minutes)\n",
        "dataset = dataset.map(prepare_dataset, remove_columns=dataset.column_names)\n",
        "\n",
        "# Filter out examples longer than 200 tokens\n",
        "def is_not_too_long(input_ids):\n",
        "    input_length = len(input_ids)\n",
        "    return input_length < 200\n",
        "\n",
        "dataset = dataset.filter(is_not_too_long, input_columns=[\"input_ids\"])\n",
        "print(len(dataset))\n",
        "\n",
        "# Create train/test split\n",
        "dataset = dataset.train_test_split(test_size=0.1)\n",
        "\n",
        "# Define custom data collator\n",
        "from dataclasses import dataclass\n",
        "from typing import Any, Dict, List, Union\n",
        "import torch\n",
        "\n",
        "@dataclass\n",
        "class TTSDataCollatorWithPadding:\n",
        "    processor: Any\n",
        "\n",
        "    def __call__(\n",
        "        self, features: List[Dict[str, Union[List[int], torch.Tensor]]]\n",
        "    ) -> Dict[str, torch.Tensor]:\n",
        "        input_ids = [{\"input_ids\": feature[\"input_ids\"]} for feature in features]\n",
        "        label_features = [{\"input_values\": feature[\"labels\"]} for feature in features]\n",
        "        speaker_features = [feature[\"speaker_embeddings\"] for feature in features]\n",
        "\n",
        "        # Collate the inputs and targets into a batch\n",
        "        batch = processor.pad(\n",
        "            input_ids=input_ids, labels=label_features, return_tensors=\"pt\"\n",
        "        )\n",
        "\n",
        "        # Replace padding with -100 to ignore loss correctly\n",
        "        batch[\"labels\"] = batch[\"labels\"].masked_fill(\n",
        "            batch.decoder_attention_mask.unsqueeze(-1).ne(1), -100\n",
        "        )\n",
        "\n",
        "        # Not used during fine-tuning\n",
        "        del batch[\"decoder_attention_mask\"]\n",
        "\n",
        "        # Round down target lengths to multiple of reduction factor\n",
        "        if model.config.reduction_factor > 1:\n",
        "            target_lengths = torch.tensor(\n",
        "                [len(feature[\"input_values\"]) for feature in label_features]\n",
        "            )\n",
        "            target_lengths = target_lengths.new(\n",
        "                [\n",
        "                    length - length % model.config.reduction_factor\n",
        "                    for length in target_lengths\n",
        "                ]\n",
        "            )\n",
        "            max_length = max(target_lengths)\n",
        "            batch[\"labels\"] = batch[\"labels\"][:, :max_length]\n",
        "\n",
        "        # Add speaker embeddings\n",
        "        batch[\"speaker_embeddings\"] = torch.tensor(speaker_features)\n",
        "\n",
        "        return batch\n",
        "\n",
        "# Initialize data collator\n",
        "data_collator = TTSDataCollatorWithPadding(processor=processor)"
      ],
      "metadata": {
        "id": "0HAOdUfBOgT6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vUK9BeOSO0u0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Model Training**\n",
        "Fine-tune pre-trained SpeechT5 model on Dutch dataset using optimized training configuration.\n",
        "Key Steps\n",
        "1. Model Loading: Load pre-trained checkpoint\n",
        "2. Cache Configuration: Disable cache for training, enable for inference\n",
        "3. Training Setup: Configure parameters and initialize trainer\n",
        "4. Training Execution: Run training and push to Hub"
      ],
      "metadata": {
        "id": "-xTlzWgpO1Ro"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load pre-trained model\n",
        "from transformers import SpeechT5ForTextToSpeech\n",
        "\n",
        "model = SpeechT5ForTextToSpeech.from_pretrained(checkpoint)\n",
        "\n",
        "# Configure cache settings\n",
        "from functools import partial\n",
        "\n",
        "# Disable cache during training (incompatible with gradient checkpointing)\n",
        "model.config.use_cache = False\n",
        "\n",
        "# Re-enable cache for generation to speed up inference\n",
        "model.generate = partial(model.generate, use_cache=True)\n",
        "\n",
        "# Define training arguments\n",
        "from transformers import Seq2SeqTrainingArguments\n",
        "\n",
        "training_args = Seq2SeqTrainingArguments(\n",
        "    output_dir=\"Denhotech/tts\",\n",
        "    per_device_train_batch_size=4,\n",
        "    gradient_accumulation_steps=8,\n",
        "    learning_rate=1e-5,\n",
        "    warmup_steps=500,\n",
        "    max_steps=4000,\n",
        "    gradient_checkpointing=True,\n",
        "    fp16=True,\n",
        "    eval_strategy=\"steps\",\n",
        "    per_device_eval_batch_size=2,\n",
        "    save_steps=100,\n",
        "    eval_steps=100,\n",
        "    logging_steps=25,\n",
        "    report_to=[\"tensorboard\"],\n",
        "    load_best_model_at_end=True,\n",
        "    greater_is_better=False,\n",
        "    label_names=[\"labels\"],\n",
        "    push_to_hub=True,\n",
        ")\n",
        "\n",
        "# Initialize trainer\n",
        "from transformers import Seq2SeqTrainer\n",
        "\n",
        "trainer = Seq2SeqTrainer(\n",
        "    args=training_args,\n",
        "    model=model,\n",
        "    train_dataset=dataset[\"train\"],\n",
        "    eval_dataset=dataset[\"test\"],\n",
        "    data_collator=data_collator,\n",
        "    tokenizer=processor,\n",
        ")\n",
        "\n",
        "# Start training (takes several hours)\n",
        "trainer.train()\n",
        "\n",
        "# Push final model to Hub\n",
        "trainer.push_to_hub()"
      ],
      "metadata": {
        "id": "q7ReUWnIPerB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}