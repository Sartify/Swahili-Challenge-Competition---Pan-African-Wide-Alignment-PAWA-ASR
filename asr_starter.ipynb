{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x19q6k0cAV3f"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yjch8qtYAfip"
   },
   "source": [
    "## Fine tune OpenAI Whisper model\n",
    "\n",
    "This notebook demonstrates how to fine-tune OpenAI's Whisper model for Swahili speech recognition. We'll take a pre-trained Whisper model and adapt it to better understand Swahili audio, improving its accuracy for this specific language.\n",
    "\n",
    "In this notebook the following steps will be followed\n",
    "1. **Environment Setup** - Install required packages and authenticate\n",
    "2. **Data Loading** - Load and inspect the Swahili audio dataset\n",
    "3. **Data Preprocessing** - Convert audio and text to model-compatible format\n",
    "4. **Model Configuration** - Set up the pre-trained Whisper model\n",
    "5. **Training Setup** - Configure training parameters and metrics\n",
    "6. **Fine-tuning** - Train the model on Swahili data\n",
    "7. **Evaluation** - Assess model performance using Word Error Rate (WER)\n",
    "8. **Model Deployment** - Save and upload the trained model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5sYDuhz9A7af"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v9ng7GHKBkzx"
   },
   "source": [
    "## Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h_AJoytzBmrX"
   },
   "outputs": [],
   "source": [
    "# Disable experiment tracking for cleaner output\n",
    "import os\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "\n",
    "# Install required packages for speech recognition training\n",
    "!pip install datasets transformers torch evaluate jiwer accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pnXi7JLNB08i"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Import necessary libraries\n",
    "from datasets import load_dataset, Audio\n",
    "from transformers import (\n",
    "    WhisperProcessor,\n",
    "    WhisperForConditionalGeneration,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    Seq2SeqTrainer\n",
    ")\n",
    "import torch\n",
    "import evaluate\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, List, Union\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l6HO3AIZCCyx"
   },
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9S8pcBNGCEsD"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Load the Swahili speech dataset from Hugging Face Hub\n",
    "# This dataset contains audio files paired with their transcriptions\n",
    "dataset = load_dataset(\"Denhotech/data_hf1\")\n",
    "\n",
    "print(f\"Dataset loaded successfully!\")\n",
    "print(f\"Training samples: {len(dataset['train'])}\")\n",
    "print(f\"Test samples: {len(dataset['test'])}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NJqd8g-6CLOf"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QgxKEdI-COco"
   },
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Qplfc_SJCPpt"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Ensure all audio is sampled at 16kHz (Whisper's required sampling rate)\n",
    "sample_audio = dataset[\"train\"][0][\"audio\"]\n",
    "if sample_audio['sampling_rate'] != 16000:\n",
    "    print(\"Resampling audio to 16kHz...\")\n",
    "    dataset = dataset.cast_column(\"audio\", Audio(sampling_rate=16000))\n",
    "else:\n",
    "    print(\"Audio already at correct sampling rate (16kHz)\")\n",
    "\n",
    "# Initialize the Whisper processor for Swahili\n",
    "# This handles both audio feature extraction and text tokenization\n",
    "processor = WhisperProcessor.from_pretrained(\n",
    "    \"openai/whisper-small\",\n",
    "    language=\"swahili\",\n",
    "    task=\"transcribe\"\n",
    ")\n",
    "\n",
    "def prepare_dataset(batch):\n",
    "    \"\"\"\n",
    "    Convert raw audio and text into model inputs\n",
    "    - Audio becomes input_features (mel spectrograms)\n",
    "    - Text becomes labels (token IDs)\n",
    "    \"\"\"\n",
    "    # Convert audio waveform to mel-spectrogram features\n",
    "    audio = batch[\"audio\"]\n",
    "    batch[\"input_features\"] = processor(\n",
    "        audio[\"array\"],\n",
    "        sampling_rate=audio[\"sampling_rate\"]\n",
    "    ).input_features[0]\n",
    "\n",
    "    # Convert text to token IDs (handle different column names)\n",
    "    text_col = \"text\" if \"text\" in batch else \"sentence\"\n",
    "    batch[\"labels\"] = processor.tokenizer(batch[text_col]).input_ids\n",
    "\n",
    "    return batch\n",
    "\n",
    "# Apply preprocessing to the entire dataset\n",
    "print(\"Processing dataset...\")\n",
    "dataset = dataset.map(prepare_dataset, remove_columns=dataset[\"train\"].column_names)\n",
    "print(\"Dataset preprocessing complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mj_zfHu7Cd0M"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BOqD2JdiCijk"
   },
   "source": [
    "## Data Collation Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XxZTX4MLCk4M"
   },
   "outputs": [],
   "source": [
    "\n",
    "@dataclass\n",
    "class DataCollator:\n",
    "    \"\"\"\n",
    "    Handles batching of variable-length audio and text sequences\n",
    "    Ensures all sequences in a batch have the same length through padding\n",
    "    \"\"\"\n",
    "    processor: any\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "        # Pad audio features to same length within batch\n",
    "        input_features = [{\"input_features\": f[\"input_features\"]} for f in features]\n",
    "        batch = self.processor.feature_extractor.pad(input_features, return_tensors=\"pt\")\n",
    "\n",
    "        # Pad text labels to same length within batch\n",
    "        label_features = [{\"input_ids\": f[\"labels\"]} for f in features]\n",
    "        labels_batch = self.processor.tokenizer.pad(label_features, return_tensors=\"pt\")\n",
    "\n",
    "        # Replace padding tokens with -100 (ignored during loss calculation)\n",
    "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
    "\n",
    "        # Remove beginning-of-sequence token if present (not needed for Whisper training)\n",
    "        if (labels[:, 0] == self.processor.tokenizer.bos_token_id).all().cpu().item():\n",
    "            labels = labels[:, 1:]\n",
    "\n",
    "        batch[\"labels\"] = labels\n",
    "        return batch\n",
    "\n",
    "# Create data collator instance\n",
    "data_collator = DataCollator(processor=processor)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wqTiH1gDCzYi"
   },
   "source": [
    "## Model Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nqtEZj6-C02a"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Load the pre-trained Whisper-small model\n",
    "# This model already knows how to transcribe speech but we'll adapt it for Swahili\n",
    "print(\"Loading pre-trained Whisper model...\")\n",
    "model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-small\")\n",
    "model.config.use_cache = False  # Disable caching for training efficiency\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o9WRfFRgC7Vz"
   },
   "source": [
    "## Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IN2IRm8RC_sg"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Load Word Error Rate (WER) metric - standard for speech recognition evaluation\n",
    "# Lower WER means better performance (0% = perfect transcription)\n",
    "from jiwer import wer, RemovePunctuation, RemoveDiacritics, ToLowerCase, RemoveMultipleSpaces\n",
    "\n",
    "# Define the transformation for WER computation\n",
    "transformation = (\n",
    "    RemovePunctuation() \n",
    "    | RemoveDiacritics()\n",
    "    | ToLowerCase()\n",
    "    | RemoveMultipleSpaces()\n",
    ")\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    pred_ids, label_ids = eval_pred\n",
    "    label_ids[label_ids == -100] = processor.tokenizer.pad_token_id\n",
    "\n",
    "    pred_str = processor.tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "    label_str = processor.tokenizer.batch_decode(label_ids, skip_special_tokens=True)\n",
    "\n",
    "    wer_score = 100 * wer(\n",
    "        ground_truth=label_str,\n",
    "        hypothesis=pred_str,\n",
    "        truth_transform=transformation,\n",
    "        hypothesis_transform=transformation,\n",
    "    )\n",
    "    return {\"wer\": wer_score}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WbWEWNCjDEui"
   },
   "source": [
    "## Training Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oZM2fubTDJjO"
   },
   "outputs": [],
   "source": [
    "# Configure training parameters\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"whisper-swahili\",\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=4,\n",
    "    gradient_accumulation_steps=2,\n",
    "    learning_rate=1e-5,\n",
    "    warmup_steps=100,\n",
    "    num_train_epochs=3,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=500,\n",
    "    save_steps=500,\n",
    "    logging_steps=50,\n",
    "    fp16=True,\n",
    "    predict_with_generate=True,\n",
    "    remove_unused_columns=False,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"wer\",\n",
    "    greater_is_better=False,\n",
    "    push_to_hub=True,\n",
    "    hub_model_id=\"Denhotech/asr_model\",\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# Initialize the trainer with model, data, and configuration\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    eval_dataset=dataset[\"test\"],\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    processing_class=processor,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2uoTEazMDtWy"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "srP5hgeFDwwb"
   },
   "source": [
    "## Model Fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xBKbi5zRDy1T"
   },
   "outputs": [],
   "source": [
    "print(\"Starting fine-tuning process...\")\n",
    "\n",
    "# Begin training - this adapts the pre-trained model to Swahili speech\n",
    "trainer.train()\n",
    "\n",
    "print(\"Fine-tuning completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ETprAPl6D5hf"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OeBt7TR0D_pH"
   },
   "source": [
    "## Model Saving and Deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2dT2xNXhEBI8"
   },
   "outputs": [],
   "source": [
    "\n",
    "print(\"Saving trained model...\")\n",
    "\n",
    "# Save the fine-tuned model and processor locally\n",
    "trainer.save_model()\n",
    "processor.save_pretrained(\"whisper-swahili\")\n",
    "\n",
    "# Upload the trained model to Hugging Face Hub for sharing\n",
    "print(\"Uploading to Hugging Face Hub...\")\n",
    "trainer.push_to_hub()\n",
    "processor.push_to_hub(\"Denhotech/asr_model\")\n",
    "\n",
    "print(\"Model successfully uploaded to: https://huggingface.co/Denhotech/asr_model\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Inference and Submission File Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torchaudio\n",
    "from transformers import WhisperProcessor, WhisperForConditionalGeneration\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "# Load the fine-tuned model \n",
    "processor = WhisperProcessor.from_pretrained(\"whisper-swahili\")\n",
    "model = WhisperForConditionalGeneration.from_pretrained(\"whisper-swahili\")\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# Path to held-out validation dataset in CSV\n",
    "# You need a CSV with at least two columns: `id` and `audio_filepath`\n",
    "# e.g. validation.csv:\n",
    "# id,audio_filepath\n",
    "# 001,validation_audio/001.wav\n",
    "# 002,validation_audio/002.wav\n",
    "\n",
    "validation_csv_path = \"validation.csv\"  # 🔁 Replace with real path\n",
    "validation_df = pd.read_csv(validation_csv_path)\n",
    "\n",
    "# Resample helper if needed\n",
    "resampler = torchaudio.transforms.Resample(orig_freq=48000, new_freq=16000)  # adjust if input is 48kHz\n",
    "\n",
    "# Inference loop\n",
    "predictions = []\n",
    "\n",
    "print(\"Running inference on validation set...\")\n",
    "\n",
    "for idx, row in tqdm(validation_df.iterrows(), total=len(validation_df)):\n",
    "    audio_path = row[\"audio_filepath\"]\n",
    "    waveform, sample_rate = torchaudio.load(audio_path)\n",
    "\n",
    "    # Optional resampling to 16kHz\n",
    "    if sample_rate != 16000:\n",
    "        waveform = resampler(waveform)\n",
    "\n",
    "    # Whisper expects mono input\n",
    "    if waveform.shape[0] > 1:\n",
    "        waveform = torch.mean(waveform, dim=0, keepdim=True)\n",
    "\n",
    "    input_features = processor(\n",
    "        waveform.squeeze().numpy(),\n",
    "        sampling_rate=16000,\n",
    "        return_tensors=\"pt\"\n",
    "    ).input_features.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        predicted_ids = model.generate(input_features)\n",
    "\n",
    "    transcription = processor.batch_decode(predicted_ids, skip_special_tokens=True)[0]\n",
    "\n",
    "    predictions.append({\n",
    "        \"id\": row[\"id\"],\n",
    "        \"predicted_text\": transcription\n",
    "    })\n",
    "\n",
    "#Save to CSV in submission format\n",
    "submission_df = pd.DataFrame(predictions)\n",
    "submission_df.to_csv(\"submission_predictions.csv\", index=False)\n",
    "\n",
    "print(\"✅ Inference complete! Saved to submission_predictions.csv\")\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
